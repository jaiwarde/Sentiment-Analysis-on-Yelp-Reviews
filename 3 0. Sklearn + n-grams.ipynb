{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import random\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "import pickle\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "from nltk.classify import ClassifierI\n",
    "from statistics import mode\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import all the necessary csv files\n",
    "parent_dir = 'D:/DCU Documents/Semester 2/Data Analytics and Data Mining/Project/Kaggle Dataset/'\n",
    "\n",
    "path_dic = {\n",
    "            'B':parent_dir+'business_s.csv',\n",
    "            'R':parent_dir+'Review2021.csv',\n",
    "            'U':parent_dir+'U.csv',\n",
    "            'D':parent_dir+'documents.csv'\n",
    "           }\n",
    "\n",
    "documents = pd.read_csv(path_dic['D'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents.columns\n",
    "documents.drop(documents.columns[[0]], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = documents. to_records(index=False)\n",
    "documents = list(documents)\n",
    "#print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "all_words = \"\"\n",
    "Stemmed_data = []\n",
    "for (t, star) in documents:\n",
    "    for word in t.split():\n",
    "        w = word.lower().replace('.', '').replace(',', '').replace('!', '')\n",
    "        all_words = all_words + ps.stem(w) +\" \"\n",
    "    Stemmed_data.append((all_words,star))\n",
    "    all_words = \"\"\n",
    "        \n",
    "#all_words = nltk.FreqDist(all_words)\n",
    "#print(all_words.most_common(15))\n",
    "#print(\"stupid appeared: \" + str(all_words['stupid']) + \" times\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"sorri but it wa not good we order chicken marsala d- it wa suppos to be creami it wasn't no prosciutto dri chicken gyro f+ meat wa dri sauc on the side pita pizza c too much sauc but the babi like it burger b+ but you can get a burger anywher the place had a cigarett or mold type of smell but i do have to say in the begin we felt kind of rush but our second waitress if you i had to go back and trust me i won't it will have to be for her servic \",\n",
       " 2.0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Stemmed_data[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"They can't manage to get online orders right and no one there answers the phone to be able to correct them. They don't even have a voicemail to leave a message.\", 1.),\n",
       " (\"The hamburger and fries were surprisingly delicious.  Great value for the money.  My partner had the burger and onion rings.  The rings were way above expectations.   I'll get the next time I'm here.\", 4.),\n",
       " (\"Sorry but it was not good we order chicken Marsala D- it was supposed to be creamy it wasn't \\nNo prosciutto dry chicken \\nGyro F+ meat was dry sauce on the side \\nPita pizza c too much sauce but the baby like it\\nBurger B+ but you can get a burger anywhere \\nThe place had a cigarette or mold type of smell \\nBut I do have to say in the beginning we felt kind of Rush but our second waitress if you I had to go back and trust me I won't it will have to be for her service!!!!\", 2.)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(Stemmed_data, columns=['text_stem', 'star'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"pos\"][8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df[\"star\"] != 3.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df.star >= 4, 'pos'] = 1\n",
    "df.loc[df.star <= 2, 'pos'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train:  (94367,)\n",
      "Shape of X_test:  (31456,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split((df['text_stem']), (df['pos']), train_size = .75, random_state = 47)\n",
    "print(\"Shape of X_train: \", X_train.shape)\n",
    "print(\"Shape of X_test: \", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['00',\n",
       " 'bacchanalia',\n",
       " 'chewabl',\n",
       " 'disinfect',\n",
       " 'funfetti',\n",
       " 'instructor',\n",
       " 'marriott',\n",
       " 'oysters',\n",
       " 'religi',\n",
       " 'soltani',\n",
       " 'true']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer(max_df = 0.95, min_df = 3, stop_words = 'english').fit(X_train)\n",
    "vect.get_feature_names()[::2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of features:  21857\n"
     ]
    }
   ],
   "source": [
    "print(\"Total number of features: \", len(vect.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<94367x21857 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 3515648 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we vectorize the X_train data\n",
    "X_train_vectorized = vect.transform(X_train)\n",
    "X_train_vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 7585   938]\n",
      " [  810 22123]]\n",
      "Test accuracy:  0.9444303153611394\n",
      "AUC:  0.9273122871399675\n"
     ]
    }
   ],
   "source": [
    "clf = LinearSVC(max_iter=1200000)\n",
    "clf.fit(X_train_vectorized, y_train)\n",
    "\n",
    "pred = clf.predict(vect.transform(X_test))\n",
    "\n",
    "print(confusion_matrix(y_true = y_test, y_pred = pred))\n",
    "print(\"Test accuracy: \", (confusion_matrix(y_true = y_test, y_pred = pred)[0][0] + confusion_matrix(y_true = y_test, y_pred = pred)[1][1])/len(X_test))\n",
    "print('AUC: ', roc_auc_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smallest Coefs: \n",
      "['mislead' 'quincy' '1909' 'emul' 'caller' 'banner' 'pathet' 'basics'\n",
      " 'brunt' 'unbias' 'likes' 'tediou' 'rudest' 'tractor' 'paddlefish']\n",
      "\n",
      "Biggest Coefs: \n",
      "['sammy' 'daddi' 'appoin' 'murphy' 'magnific' 'panhandl' 'ingredients'\n",
      " 'dynasti' 'graciou' 'phenomen' 'sprint' 'seconds' 'avo' 'started' 'caraf']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "feature_names = np.array(vect.get_feature_names())\n",
    "\n",
    "sored_coef_index = clf.coef_[0].argsort()\n",
    "print(\"Smallest Coefs: \\n{}\\n\".format(feature_names[sored_coef_index[:15]]))\n",
    "print(\"Biggest Coefs: \\n{}\\n\".format(feature_names[sored_coef_index[:-16:-1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tfâ€“idf, or Term frequency-inverse document frequency, allows us to weight terms based on how important they are to a document. High weight is given to terms that appear often in a particular document, but don't appear often in the corpus. Features with low tfâ€“idf are either commonly used across all documents or rarely used and only occur in long documents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21857"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = TfidfVectorizer(max_df = .95, min_df = 3, stop_words = 'english').fit(X_train)\n",
    "len(vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 7739   784]\n",
      " [  604 22329]]\n",
      "Test accuracy:  0.9558748728382502\n",
      "AUC:  0.9408380090574874\n"
     ]
    }
   ],
   "source": [
    "X_train_vectorized = vect.transform(X_train)\n",
    "\n",
    "clf = LinearSVC()\n",
    "clf.fit(X_train_vectorized, y_train)\n",
    "\n",
    "pred = clf.predict(vect.transform(X_test))\n",
    "\n",
    "print(confusion_matrix(y_true = y_test, y_pred = pred))\n",
    "print(\"Test accuracy: \", (confusion_matrix(y_true = y_test, y_pred = pred)[0][0] + confusion_matrix(y_true = y_test, y_pred = pred)[1][1])/len(X_test))\n",
    "print('AUC: ', roc_auc_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smallest Coefs: \n",
      "['worst' 'bland' 'terribl' 'horribl' 'rude' 'disgust' 'unaccept' 'mediocr'\n",
      " 'unprofession' 'aw' 'tasteless' 'poor' 'mislead' 'scam' 'wors'\n",
      " 'flavorless' 'downhil' 'unfortun' 'underwhelm' 'zero' 'bare' 'subpar'\n",
      " 'lack' 'insult' 'stale']\n",
      "\n",
      "Biggest Coefs: \n",
      "['delici' 'amaz' 'great' 'excel' 'perfect' 'best' 'love' 'outstand'\n",
      " 'perfectli' 'phenomen' 'thank' 'fantast' 'awesom' 'highli' 'appreci'\n",
      " 'glad' 'friendli' 'yummi' 'tasti' 'good' 'thorough' 'favorit'\n",
      " 'profession' 'gem' 'grate']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "feature_names = np.array(vect.get_feature_names())\n",
    "\n",
    "sored_coef_index = clf.coef_[0].argsort()\n",
    "print(\"Smallest Coefs: \\n{}\\n\".format(feature_names[sored_coef_index[:25]]))\n",
    "print(\"Biggest Coefs: \\n{}\\n\".format(feature_names[sored_coef_index[:-26:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1.]\n"
     ]
    }
   ],
   "source": [
    "# The issue with n-grams: do not, not recommend, not good\n",
    "print(clf.predict(vect.transform(['do not recommend this place',\n",
    "                                 'this place is not good'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "293979\n"
     ]
    }
   ],
   "source": [
    "vect = TfidfVectorizer(max_df = .95, min_df = 3, ngram_range = (1,2)).fit(X_train)\n",
    "print(len(vect.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 8042   481]\n",
      " [  399 22534]]\n",
      "Test accuracy:  0.9720244150559512\n",
      "AUC:  0.9630829819521445\n"
     ]
    }
   ],
   "source": [
    "X_train_vectorized = vect.transform(X_train)\n",
    "\n",
    "clf = LinearSVC()\n",
    "clf.fit(X_train_vectorized, y_train)\n",
    "\n",
    "pred = clf.predict(vect.transform(X_test))\n",
    "\n",
    "print(confusion_matrix(y_true = y_test, y_pred = pred))\n",
    "print(\"Test accuracy: \", (confusion_matrix(y_true = y_test, y_pred = pred)[0][0] + confusion_matrix(y_true = y_test, y_pred = pred)[1][1])/len(X_test))\n",
    "print('AUC: ', roc_auc_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smallest Coefs: \n",
      "['not' 'rude' 'worst' 'terribl' 'veri disappoint' 'horribl' 'bland' 'no'\n",
      " 'disappoint' 'poor' 'disgust' 'not worth' 'overpr' 'not good' 'mediocr'\n",
      " 'aw' 'unfortun' 'not recommend' 'bare' 'at best' 'wors' 'unprofession'\n",
      " 'lack' 'bad' 'two star' 'the worst' 'told' 'elsewher' 'never' 'money'\n",
      " 'dri' 'zero' 'do not' 'don' 'won' 'shame' 'cold' 'unaccept' 'trash'\n",
      " 'scam' 'will not' 'ridicul' 'so disappoint' 'wouldn recommend' 'subpar']\n",
      "\n",
      "Biggest Coefs: \n",
      "['great' 'amaz' 'delici' 'love' 'best' 'excel' 'not disappoint' 'good'\n",
      " 'thank' 'perfect' 'be disappoint' 'awesom' 'never disappoint' 'fantast'\n",
      " 'definit' 'appreci' 'friendli' 'tasti' 'the best' 'outstand' 'nice'\n",
      " 'perfectli' 'highli recommend' 'happi' 'beauti' 'profession' 'glad'\n",
      " 'recommend' 'favorit' 'comfort' 'thank you' 'effici' 'alway' 'easi'\n",
      " 'so good' 'enjoy' 'not bad' 'highli' 'will definit' 'yummi' 'fresh'\n",
      " 'help' 'wonder' 'and' 'wa abl']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "feature_names = np.array(vect.get_feature_names())\n",
    "\n",
    "sored_coef_index = clf.coef_[0].argsort()\n",
    "print(\"Smallest Coefs: \\n{}\\n\".format(feature_names[sored_coef_index[:45]]))\n",
    "print(\"Biggest Coefs: \\n{}\\n\".format(feature_names[sored_coef_index[:-46:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_words = pd.DataFrame(feature_names[sored_coef_index[:200]], columns = ['positive_words'])\n",
    "pos_words.to_csv(parent_dir+'Positive Words.csv')\n",
    "neg_words = pd.DataFrame(feature_names[sored_coef_index[:-200:-1]], columns = ['negative_words'])\n",
    "neg_words.to_csv(parent_dir+'Negative Words.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# NO MORE issue with n-grams: do not, not recommend, not good\n",
    "print(clf.predict(vect.transform(['do not recommend this place',\n",
    "                                 'this place is not good'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying the voting clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression accuracy:  0.9640132248219736\n",
      "[[ 8223   300]\n",
      " [  832 22101]]\n",
      "AUC:  0.9642607646383946\n",
      "LinearSVC accuracy:  0.9720244150559512\n",
      "[[ 8042   481]\n",
      " [  399 22534]]\n",
      "AUC:  0.9630829819521445\n",
      "SGDClassifier accuracy:  0.958736012207528\n",
      "[[ 8263   260]\n",
      " [ 1038 21895]]\n",
      "AUC:  0.9621160118631955\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(47)\n",
    "# LogisticRegression\n",
    "clf_LogisticRegression = LogisticRegression(max_iter=1200000,class_weight = 'balanced') #class_weight = 'balanced'\n",
    "clf_LogisticRegression.fit(X_train_vectorized, y_train)\n",
    "\n",
    "pred = clf_LogisticRegression.predict(vect.transform(X_test))\n",
    "print(\"LogisticRegression accuracy: \", (confusion_matrix(y_true = y_test, y_pred = pred)[0][0] + confusion_matrix(y_true = y_test, y_pred = pred)[1][1])/len(X_test))\n",
    "print(confusion_matrix(y_true = y_test, y_pred = pred))\n",
    "print('AUC: ', roc_auc_score(y_test, pred))\n",
    "\n",
    "# LinearSVC\n",
    "clf_LinearSVC = LinearSVC() #class_weight = 'balanced'\n",
    "clf_LinearSVC.fit(X_train_vectorized, y_train)\n",
    "\n",
    "pred = clf_LinearSVC.predict(vect.transform(X_test))\n",
    "print(\"LinearSVC accuracy: \", (confusion_matrix(y_true = y_test, y_pred = pred)[0][0] + confusion_matrix(y_true = y_test, y_pred = pred)[1][1])/len(X_test))\n",
    "print(confusion_matrix(y_true = y_test, y_pred = pred))\n",
    "print('AUC: ', roc_auc_score(y_test, pred))\n",
    "\n",
    "# SGDClassifier\n",
    "clf_SGDClassifier = SGDClassifier(class_weight = 'balanced') #class_weight = 'balanced'\n",
    "clf_SGDClassifier.fit(X_train_vectorized, y_train)\n",
    "\n",
    "pred = clf_SGDClassifier.predict(vect.transform(X_test))\n",
    "print(\"SGDClassifier accuracy: \", (confusion_matrix(y_true = y_test, y_pred = pred)[0][0] + confusion_matrix(y_true = y_test, y_pred = pred)[1][1])/len(X_test))\n",
    "print(confusion_matrix(y_true = y_test, y_pred = pred))\n",
    "print('AUC: ', roc_auc_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying out the voted clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "voted_classifier accuracy:  0.9641721770091557\n"
     ]
    }
   ],
   "source": [
    "class VoteClassifier(ClassifierI):\n",
    "    def __init__(self, *classifiers):\n",
    "        self._classifiers = classifiers\n",
    "\n",
    "    def predict(self, features):\n",
    "        pred_list = []\n",
    "        for c in self._classifiers:\n",
    "            pred_list.append(c.predict(features))\n",
    "        \n",
    "        res = []\n",
    "        for i in range(len(pred_list[0])):\n",
    "            a = pred_list[0][i]\n",
    "            b = pred_list[1][i]\n",
    "            c = pred_list[2][i]\n",
    "            if (a+b+c)< 2:\n",
    "                res.append(0)\n",
    "            else:\n",
    "                res.append(1)\n",
    "        return res\n",
    "\n",
    "    def confidence(self, features):\n",
    "        pred_list = []\n",
    "        for c in self._classifiers:\n",
    "            pred_list.append(c.predict(features))\n",
    "        \n",
    "        votes = pred_list[0][0] + pred_list[1][0] + pred_list[2][0]\n",
    "        if votes< 2:\n",
    "            return 1-votes/3\n",
    "        else:\n",
    "            return votes/3\n",
    "    \n",
    "voted_classifier = VoteClassifier(clf_LogisticRegression,\n",
    "                                  clf_LinearSVC, \n",
    "                                  clf_SGDClassifier) \n",
    "\n",
    "pred = voted_classifier.predict(vect.transform(X_test))\n",
    "print(\"voted_classifier accuracy: \", (confusion_matrix(y_true = y_test, y_pred = pred)[0][0] + confusion_matrix(y_true = y_test, y_pred = pred)[1][1])/len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voted_classifier.predict(vect.transform(['do not recommend this place',\n",
    "                                 'this place is not good']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 1.0)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sentiment(text):\n",
    "    return (voted_classifier.predict(vect.transform(text))[0], voted_classifier.confidence(vect.transform(text)))\n",
    "\n",
    "sentiment(['I do not recommend this place'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1.0)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment([\"we will come back again.\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_format(value):\n",
    "    return value.lower().replace('.', '').replace(',', '').replace('!', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['You', 'have', 'to', 'build', 'a', 'very', 'good', 'site', 'and', 'I', 'love', 'visiting', 'your', 'site', '.']\n",
      "['you', 'have', 'to', 'build', 'a', 'very', 'good', 'site', 'and', 'i', 'love', 'visiting', 'your', 'site', '']\n",
      "['you', 'build', 'good', 'site', 'i', 'love', 'visiting', 'site', '']\n",
      "['you', 'build', 'good', 'site', 'i', 'love', 'visit', 'site', '']\n"
     ]
    }
   ],
   "source": [
    "ps = PorterStemmer() \n",
    "example_sec = 'You have to build a very good site and I love visiting your site.'\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stopWords = list(set(stopwords.words('english')))\n",
    "\n",
    "print([w for w in word_tokenize(example_sec)])\n",
    "print([clean_format(w) for w in word_tokenize(example_sec)])\n",
    "print([clean_format(w) for w in word_tokenize(example_sec) if w not in stopWords])\n",
    "print([ps.stem(clean_format(w)) for w in word_tokenize(example_sec) if w not in stopWords])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "sens = [\"The python\", \"The python programmer named pythoner \", \"The python programmer named pythoner is pythoning a game pythonly\"]\n",
    "samp_list = np.array([\" \".join([ps.stem(clean_format(w)) for w in word_tokenize(example_sec) if w not in stopWords]) for example_sec in sens])\n",
    "# samp_list\n",
    "transed = vect.transform(samp_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'amaz'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps.stem('amazing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(parent_dir+\"document_pos_value.csv\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
